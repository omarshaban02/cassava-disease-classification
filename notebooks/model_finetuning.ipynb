{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4a7a5e1",
   "metadata": {},
   "source": [
    "# Cassava Leaf Disease Classification Using Deep Learning Models\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "### 1.1 Problem Statement\n",
    "This research focuses on automated detection and classification of cassava leaf diseases using deep learning approaches. Cassava, a vital food security crop in Africa, faces significant yield losses due to various diseases. Early and accurate disease detection is crucial for effective crop management.\n",
    "\n",
    "### 1.2 Dataset Overview\n",
    "We utilize the [Cassava Leaf Disease Classification](https://www.kaggle.com/c/cassava-leaf-disease-classification) dataset, which contains images of cassava leaves affected by different diseases:\n",
    "- Cassava Bacterial Blight (CBB)\n",
    "- Cassava Brown Streak Disease (CBSD)\n",
    "- Cassava Green Mottle (CGM)\n",
    "- Cassava Mosaic Disease (CMD)\n",
    "- Healthy specimens\n",
    "\n",
    "### 1.3 Research Objectives\n",
    "1. Develop and compare multiple deep learning architectures for disease classification\n",
    "2. Evaluate model performance with emphasis on balanced metrics (macro-F1)\n",
    "3. Analyze model robustness and generalization capabilities\n",
    "4. Provide insights into architectural choices for plant disease detection\n",
    "\n",
    "### 1.4 Methodological Overview\n",
    "We implement and compare three state-of-the-art architectures:\n",
    "- ResNet50 with progressive unfreezing\n",
    "- EfficientNet-B0 with advanced augmentation\n",
    "- Vision Transformer (ViT) with patch-based learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9470d79",
   "metadata": {},
   "source": [
    "## Kaggle Setup\n",
    "\n",
    "First, let's set up our Kaggle environment and access the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d5547f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Kaggle dataset\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Kaggle competition dataset\n",
    "competition_name = \"cassava-leaf-disease-classification\"\n",
    "dataset_path = Path(\"/kaggle/input/cassava-leaf-disease-classification\")\n",
    "\n",
    "# Load dataset metadata\n",
    "train_df = pd.read_csv(dataset_path / \"train.csv\")\n",
    "print(\"Dataset Overview:\")\n",
    "print(f\"Total images: {len(train_df)}\")\n",
    "print(\"\\nClass distribution:\")\n",
    "print(train_df['label'].value_counts())\n",
    "\n",
    "# Map numeric labels to disease names\n",
    "label_names = {\n",
    "    0: \"Cassava Bacterial Blight (CBB)\",\n",
    "    1: \"Cassava Brown Streak Disease (CBSD)\",\n",
    "    2: \"Cassava Green Mottle (CGM)\",\n",
    "    3: \"Cassava Mosaic Disease (CMD)\",\n",
    "    4: \"Healthy\"\n",
    "}\n",
    "\n",
    "train_df['disease_name'] = train_df['label'].map(label_names)\n",
    "print(\"\\nSample data:\")\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af815929",
   "metadata": {},
   "source": [
    "## 2. Background and Literature Review\n",
    "\n",
    "### 2.1 Cassava Disease Impact\n",
    "Cassava diseases pose a significant threat to food security in developing regions. Early detection through computer vision can significantly improve crop management and yield protection.\n",
    "\n",
    "### 2.2 Deep Learning in Plant Pathology\n",
    "Recent advances in deep learning have shown promising results in plant disease detection:\n",
    "- Convolutional Neural Networks (CNNs) for image classification\n",
    "- Transfer learning for limited dataset scenarios\n",
    "- Real-time detection systems for field applications\n",
    "\n",
    "### 2.3 Technical Background\n",
    "Our implementation utilizes:\n",
    "- PyTorch and torchvision for deep learning models\n",
    "- Advanced data augmentation techniques\n",
    "- Evaluation metrics focused on balanced performance\n",
    "- Modern training strategies (progressive unfreezing, learning rate scheduling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98e73d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torchvision import datasets, models, transforms\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from PIL import Image\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set device - Kaggle provides GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create directories for saving models and results\n",
    "os.makedirs(\"/kaggle/working/models\", exist_ok=True)\n",
    "os.makedirs(\"/kaggle/working/results\", exist_ok=True)\n",
    "\n",
    "# Competition dataset paths\n",
    "DATASET_PATH = Path(\"/kaggle/input/cassava-leaf-disease-classification\")\n",
    "TRAIN_PATH = DATASET_PATH / \"train_images\"\n",
    "TEST_PATH = DATASET_PATH / \"test_images\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d02641f",
   "metadata": {},
   "source": [
    "## 3. Methodology\n",
    "\n",
    "### 3.1 Data Preparation and Analysis\n",
    "We implement a robust data pipeline including:\n",
    "1. Custom dataset implementation\n",
    "2. Advanced augmentation strategies\n",
    "3. Stratified train/validation splitting\n",
    "4. Batch processing optimization\n",
    "\n",
    "The following section details our data preparation approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646748d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class for Cassava\n",
    "class CassavaDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.df.iloc[idx]['image_id']\n",
    "        label = self.df.iloc[idx]['label']\n",
    "        \n",
    "        # Load image\n",
    "        img_path = TRAIN_PATH / img_name\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label\n",
    "\n",
    "# Define data transforms\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Split data into train and validation\n",
    "train_df, val_df = train_test_split(\n",
    "    train_df, \n",
    "    test_size=0.2, \n",
    "    stratify=train_df['label'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CassavaDataset(train_df, transform=data_transforms['train'])\n",
    "val_dataset = CassavaDataset(val_df, transform=data_transforms['val'])\n",
    "\n",
    "# Create data loaders (we will optionally replace the train loader with a WeightedRandomSampler)\n",
    "batch_size = 32\n",
    "\n",
    "# Compute class distribution and class weights\n",
    "class_counts = train_df['label'].value_counts().sort_index()\n",
    "print(\"Training class distribution:\\n\", class_counts)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.barplot(x=class_counts.index, y=class_counts.values, palette='viridis')\n",
    "plt.xticks(ticks=np.arange(len(class_counts)), labels=[label_names[i] for i in class_counts.index], rotation=45, ha='right')\n",
    "plt.title('Original Training Set Class Distribution')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute class weights (balanced)\n",
    "classes = np.unique(train_df['label'])\n",
    "class_weights_np = compute_class_weight(class_weight='balanced', classes=classes, y=train_df['label'].values)\n",
    "class_weights = torch.tensor(class_weights_np, dtype=torch.float)\n",
    "print('Class weights (balanced):', class_weights_np)\n",
    "\n",
    "# Option: use WeightedRandomSampler to balance batches\n",
    "use_weighted_sampler = True\n",
    "if use_weighted_sampler:\n",
    "    # For the sampler we need a weight per sample (based on its label)\n",
    "    sample_weights = [class_weights_np[label] for label in train_df['label'].tolist()]\n",
    "    sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler, num_workers=2)\n",
    "    print('Using WeightedRandomSampler for train_loader')\n",
    "else:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# Display sample images\n",
    "def show_batch(loader, num_images=8):\n",
    "    batch = next(iter(loader))\n",
    "    images, labels = batch\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(15, 8))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < min(num_images, len(images)):\n",
    "            img = images[i].permute(1, 2, 0).numpy()\n",
    "            img = img * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]  # Denormalize\n",
    "            img = np.clip(img, 0, 1)\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "            ax.set_title(label_names[labels[i].item()])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nShowing a batch of training images with augmentation:\")\n",
    "show_batch(train_loader)\n",
    "\n",
    "# Verify class distribution after augmentation/sampling\n",
    "def plot_augmented_distribution(train_loader, num_classes, label_names, num_batches=100):\n",
    "    \"\"\"\n",
    "    Plot the class distribution after augmentation by sampling from the train_loader\n",
    "    \n",
    "    Args:\n",
    "        train_loader: DataLoader with augmentation/sampling\n",
    "        num_classes: Number of classes\n",
    "        label_names: Dictionary mapping class indices to names\n",
    "        num_batches: Number of batches to sample (default=100)\n",
    "    \"\"\"\n",
    "    augmented_counts = np.zeros(num_classes)\n",
    "    \n",
    "    # Sample batches and count classes\n",
    "    print(\"\\nSampling batches to verify class distribution...\")\n",
    "    for i, (_, labels) in enumerate(tqdm(train_loader, total=num_batches)):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        labels_np = labels.cpu().numpy()\n",
    "        for label in range(num_classes):\n",
    "            augmented_counts[label] += np.sum(labels_np == label)\n",
    "    \n",
    "    # Plot original vs augmented distribution\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n",
    "    \n",
    "    # Original distribution\n",
    "    original_counts = train_df['label'].value_counts().sort_index()\n",
    "    sns.barplot(x=np.arange(num_classes), y=original_counts, ax=ax1, palette='viridis')\n",
    "    ax1.set_title('Original Class Distribution')\n",
    "    ax1.set_xticks(np.arange(num_classes))\n",
    "    ax1.set_xticklabels([label_names[i] for i in range(num_classes)], rotation=45, ha='right')\n",
    "    ax1.set_ylabel('Count')\n",
    "    \n",
    "    # Augmented distribution\n",
    "    sns.barplot(x=np.arange(num_classes), y=augmented_counts, ax=ax2, palette='viridis')\n",
    "    ax2.set_title('Distribution After Augmentation/Sampling')\n",
    "    ax2.set_xticks(np.arange(num_classes))\n",
    "    ax2.set_xticklabels([label_names[i] for i in range(num_classes)], rotation=45, ha='right')\n",
    "    ax2.set_ylabel('Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print class proportions\n",
    "    print(\"\\nClass proportions after augmentation:\")\n",
    "    proportions = augmented_counts / augmented_counts.sum()\n",
    "    for i in range(num_classes):\n",
    "        print(f\"{label_names[i]}: {proportions[i]:.3f}\")\n",
    "\n",
    "# Verify class balance after augmentation/sampling\n",
    "print(\"\\nVerifying class distribution after augmentation and sampling:\")\n",
    "plot_augmented_distribution(\n",
    "    train_loader,\n",
    "    len(label_names),\n",
    "    label_names,\n",
    "    num_batches=100  # Sample 100 batches to check distribution\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f83595",
   "metadata": {},
   "source": [
    "### 3.2 Model Architecture: ResNet50\n",
    "\n",
    "#### 3.2.1 Implementation Details\n",
    "We implement ResNet50 with the following enhancements:\n",
    "1. Progressive layer unfreezing to prevent catastrophic forgetting\n",
    "2. Mixup augmentation for improved generalization\n",
    "3. Custom learning rate scheduling\n",
    "4. Advanced regularization techniques\n",
    "\n",
    "#### 3.2.2 Training Strategy\n",
    "The model is trained in multiple stages:\n",
    "- Initial stage: Only classifier training\n",
    "- Intermediate stage: Fine-tuning upper layers\n",
    "- Final stage: End-to-end fine-tuning\n",
    "\n",
    "Implementation and training code follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdf14b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixup augmentation\n",
    "def mixup_data(x, y, alpha=1.0):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "# Initialize ResNet50\n",
    "def create_resnet_model(num_classes):\n",
    "    model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "    \n",
    "    # Freeze all parameters initially\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    # Modify final classifier\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(num_features, 1024),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(1024, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(512, num_classes)\n",
    "    )\n",
    "    \n",
    "    return model.to(device)\n",
    "\n",
    "# Training utilities\n",
    "def train_epoch(model, loader, criterion, optimizer, scheduler=None, mixup=True):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for inputs, labels in tqdm(loader, leave=False):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        if mixup:\n",
    "            inputs, labels_a, labels_b, lam = mixup_data(inputs, labels)\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        if mixup:\n",
    "            loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)\n",
    "            # For metrics, use the dominant label (higher lambda weight)\n",
    "            true_labels = labels_a if lam > 0.5 else labels_b\n",
    "        else:\n",
    "            loss = criterion(outputs, labels)\n",
    "            true_labels = labels\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Store predictions and true labels for metric calculation\n",
    "        _, preds = outputs.max(1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(true_labels.cpu().numpy())\n",
    "        \n",
    "    if scheduler:\n",
    "        scheduler.step()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    # Use class_weights tensor if present to report weighted metrics if desired; here we compute accuracy and macro-F1\n",
    "    accuracy = 100. * accuracy_score(all_labels, all_preds)\n",
    "    macro_f1 = 100. * f1_score(all_labels, all_preds, average='macro')\n",
    "        \n",
    "    return running_loss / len(loader), accuracy, macro_f1\n",
    "\n",
    "def validate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, preds = outputs.max(1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = 100. * accuracy_score(all_labels, all_preds)\n",
    "    macro_f1 = 100. * f1_score(all_labels, all_preds, average='macro')\n",
    "            \n",
    "    return running_loss / len(loader), accuracy, macro_f1\n",
    "\n",
    "# Initialize model and training parameters\n",
    "num_classes = len(label_names)\n",
    "resnet_model = create_resnet_model(num_classes)\n",
    "# Use class_weights if computed; fall back to unweighted if not\n",
    "try:\n",
    "    class_weights_tensor = class_weights.to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "    print('Using class-weighted CrossEntropyLoss for ResNet')\n",
    "except NameError:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    print('Class weights not found; using standard CrossEntropyLoss')\n",
    "\n",
    "optimizer = optim.AdamW(resnet_model.fc.parameters(), lr=0.001, weight_decay=0.01)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=0.001,\n",
    "    epochs=num_epochs,\n",
    "    steps_per_epoch=len(train_loader)\n",
    ")\n",
    "\n",
    "print(\"Stage 1: Training only the classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f8b392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ResNet50\n",
    "num_epochs = 15\n",
    "best_val_f1 = 0  # Changed to track best F1 score instead of accuracy\n",
    "train_losses, val_losses = [], []\n",
    "train_acc, val_acc = [], []\n",
    "train_f1, val_f1 = [], []  # New lists for F1 scores\n",
    "\n",
    "# Training loop with progressive unfreezing\n",
    "stages = [\n",
    "    ('Initial (FC only)', ['fc']),\n",
    "    ('Stage 2 (Last block)', ['fc', 'layer4']),\n",
    "    ('Stage 3 (Last 2 blocks)', ['fc', 'layer4', 'layer3'])\n",
    "]\n",
    "\n",
    "for stage_name, layers_to_unfreeze in stages:\n",
    "    print(f\"\\n{stage_name}\")\n",
    "    \n",
    "    # Unfreeze specified layers\n",
    "    for name, param in resnet_model.named_parameters():\n",
    "        param.requires_grad = any(layer in name for layer in layers_to_unfreeze)\n",
    "    \n",
    "    # Update optimizer with unfrozen parameters\n",
    "    optimizer = optim.AdamW(\n",
    "        [p for p in resnet_model.parameters() if p.requires_grad],\n",
    "        lr=0.001,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=0.001,\n",
    "        epochs=num_epochs,\n",
    "        steps_per_epoch=len(train_loader)\n",
    "    )\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_accuracy, train_macro_f1 = train_epoch(\n",
    "            resnet_model, train_loader, criterion, optimizer, scheduler\n",
    "        )\n",
    "        val_loss, val_accuracy, val_macro_f1 = validate(resnet_model, val_loader, criterion)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_acc.append(train_accuracy)\n",
    "        val_acc.append(val_accuracy)\n",
    "        train_f1.append(train_macro_f1)\n",
    "        val_f1.append(val_macro_f1)\n",
    "        \n",
    "        if val_macro_f1 > best_val_f1:  # Save model based on F1 score\n",
    "            best_val_f1 = val_macro_f1\n",
    "            torch.save(resnet_model.state_dict(), '/kaggle/working/models/resnet50_best.pth')\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Train Macro-F1: {train_macro_f1:.2f}%')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%, Val Macro-F1: {val_macro_f1:.2f}%')\n",
    "    \n",
    "# Plot training progress\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.title('ResNet50 Loss Progress')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(train_acc, label='Train Accuracy')\n",
    "plt.plot(val_acc, label='Val Accuracy')\n",
    "plt.title('ResNet50 Accuracy Progress')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(train_f1, label='Train Macro-F1')\n",
    "plt.plot(val_f1, label='Val Macro-F1')\n",
    "plt.title('ResNet50 Macro-F1 Progress')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Macro-F1 (%)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3432c236",
   "metadata": {},
   "source": [
    "### 3.3 Model Architecture: EfficientNet-B0\n",
    "\n",
    "#### 3.3.1 Implementation Details\n",
    "EfficientNet-B0 is implemented with:\n",
    "1. Advanced data augmentation pipeline\n",
    "2. Progressive learning rate warmup\n",
    "3. Discriminative learning rates\n",
    "4. Batch normalization and dropout optimization\n",
    "\n",
    "#### 3.3.2 Training Strategy\n",
    "Our training approach includes:\n",
    "- Gradual layer unfreezing\n",
    "- Custom learning rate scheduling\n",
    "- Advanced regularization\n",
    "- Performance monitoring\n",
    "\n",
    "Implementation and training code follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758fb6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize EfficientNet\n",
    "def create_efficient_model(num_classes):\n",
    "    model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "    \n",
    "    # Freeze all parameters\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    # Modify classifier with dropout and batch norm\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.5),\n",
    "        nn.Linear(1280, 1024),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm1d(1024),\n",
    "        nn.Dropout(p=0.3),\n",
    "        nn.Linear(1024, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm1d(512),\n",
    "        nn.Dropout(p=0.2),\n",
    "        nn.Linear(512, num_classes)\n",
    "    )\n",
    "    \n",
    "    return model.to(device)\n",
    "\n",
    "class GradualWarmupScheduler(torch.optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, multiplier, warmup_epochs, total_epochs, after_scheduler=None):\n",
    "        self.multiplier = multiplier\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.total_epochs = total_epochs\n",
    "        self.after_scheduler = after_scheduler\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch < self.warmup_epochs:\n",
    "            return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.warmup_epochs + 1.) \n",
    "                    for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            if self.after_scheduler:\n",
    "                return self.after_scheduler.get_last_lr()\n",
    "            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "        self.last_epoch = epoch\n",
    "        if self.last_epoch < self.warmup_epochs:\n",
    "            for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "                param_group['lr'] = lr\n",
    "        else:\n",
    "            if self.after_scheduler:\n",
    "                self.after_scheduler.step(epoch - self.warmup_epochs)\n",
    "\n",
    "# Initialize model and training parameters\n",
    "efficient_model = create_efficient_model(num_classes)\n",
    "# Use class_weights if computed; fall back to unweighted if not\n",
    "try:\n",
    "    class_weights_tensor = class_weights.to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "    print('Using class-weighted CrossEntropyLoss for EfficientNet')\n",
    "except NameError:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    print('Class weights not found; using standard CrossEntropyLoss')\n",
    "\n",
    "# Set different learning rates for different layers\n",
    "params = [\n",
    "    {'params': efficient_model.features.parameters(), 'lr': 1e-4},\n",
    "    {'params': efficient_model.classifier.parameters(), 'lr': 1e-3}\n",
    "]\n",
    "optimizer = optim.AdamW(params, weight_decay=0.01)\n",
    "\n",
    "# Create schedulers\n",
    "main_scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, \n",
    "    T_max=num_epochs-3,\n",
    "    eta_min=1e-6\n",
    ")\n",
    "scheduler = GradualWarmupScheduler(\n",
    "    optimizer,\n",
    "    multiplier=8,\n",
    "    warmup_epochs=3,\n",
    "    total_epochs=num_epochs,\n",
    "    after_scheduler=main_scheduler\n",
    ")\n",
    "\n",
    "print(\"Starting EfficientNet training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fee524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train EfficientNet\n",
    "num_epochs = 15\n",
    "best_val_f1 = 0  # Track best F1 score\n",
    "train_losses, val_losses = [], []\n",
    "train_acc, val_acc = [], []\n",
    "train_f1, val_f1 = [], []  # New lists for F1 scores\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Gradually unfreeze layers\n",
    "    if epoch == 5:\n",
    "        print(\"Unfreezing last 2 blocks...\")\n",
    "        for param in efficient_model.features[-2:].parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "    if epoch == 10:\n",
    "        print(\"Unfreezing last 4 blocks...\")\n",
    "        for param in efficient_model.features[-4:].parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    train_loss, train_accuracy, train_macro_f1 = train_epoch(\n",
    "        efficient_model, train_loader, criterion, optimizer, None, mixup=True\n",
    "    )\n",
    "    val_loss, val_accuracy, val_macro_f1 = validate(efficient_model, val_loader, criterion)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_acc.append(train_accuracy)\n",
    "    val_acc.append(val_accuracy)\n",
    "    train_f1.append(train_macro_f1)\n",
    "    val_f1.append(val_macro_f1)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    if val_macro_f1 > best_val_f1:  # Save model based on F1 score\n",
    "        best_val_f1 = val_macro_f1\n",
    "        torch.save(efficient_model.state_dict(), '/kaggle/working/models/efficientnet_best.pth')\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Train Macro-F1: {train_macro_f1:.2f}%')\n",
    "    print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%, Val Macro-F1: {val_macro_f1:.2f}%')\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(f'Learning rate: {param_group[\"lr\"]:.6f}')\n",
    "\n",
    "# Plot training progress\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.title('EfficientNet Loss Progress')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(train_acc, label='Train Accuracy')\n",
    "plt.plot(val_acc, label='Val Accuracy')\n",
    "plt.title('EfficientNet Accuracy Progress')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(train_f1, label='Train Macro-F1')\n",
    "plt.plot(val_f1, label='Val Macro-F1')\n",
    "plt.title('EfficientNet Macro-F1 Progress')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Macro-F1 (%)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot learning rate schedule\n",
    "plt.figure(figsize=(10, 4))\n",
    "lrs = []\n",
    "optimizer.zero_grad()\n",
    "for _ in range(num_epochs):\n",
    "    lrs.append([param_group['lr'] for param_group in optimizer.param_groups])\n",
    "    scheduler.step()\n",
    "    \n",
    "plt.plot(np.array(lrs))\n",
    "plt.title('Learning Rate Schedule')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.legend(['Features', 'Classifier'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af2ef56",
   "metadata": {},
   "source": [
    "### 3.4 Model Architecture: Vision Transformer (ViT)\n",
    "\n",
    "#### 3.4.1 Implementation Details\n",
    "Our ViT implementation includes:\n",
    "1. Pretrained ViT-B/16 adaptation\n",
    "2. Patch-based image tokenization\n",
    "3. Custom head for classification\n",
    "4. Advanced positional embeddings\n",
    "\n",
    "#### 3.4.2 Training Strategy\n",
    "The training process involves:\n",
    "- Gradual fine-tuning of attention layers\n",
    "- Layer-wise learning rate decay\n",
    "- Advanced regularization techniques\n",
    "- Patch dropout for robustness\n",
    "\n",
    "Implementation and training code follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c6292f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ViT model\n",
    "def create_vit_model(num_classes):\n",
    "    \"\"\"\n",
    "    Create and initialize a Vision Transformer model with custom classification head.\n",
    "    \"\"\"\n",
    "    # Load pretrained ViT-B/16\n",
    "    model = models.vit_b_16(weights=models.ViT_B_16_Weights.IMAGENET1K_V1)\n",
    "    \n",
    "    # Modify the head for our classification task\n",
    "    num_features = model.heads.head.in_features\n",
    "    model.heads = nn.Sequential(\n",
    "        nn.LayerNorm(num_features),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(num_features, 512),\n",
    "        nn.GELU(),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.Linear(512, num_classes)\n",
    "    )\n",
    "    \n",
    "    return model.to(device)\n",
    "\n",
    "# Layer-wise learning rate decay\n",
    "def get_optimizer_params(model, weight_decay, lr_init):\n",
    "    \"\"\"\n",
    "    Apply different learning rates to different layers using decay.\n",
    "    \"\"\"\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    \n",
    "    # Separate parameters into decay and no_decay groups\n",
    "    optimizer_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer \n",
    "                if not any(nd in n for nd in no_decay) and \"heads\" not in n\n",
    "            ],\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"lr\": lr_init\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer \n",
    "                if any(nd in n for nd in no_decay) and \"heads\" not in n\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "            \"lr\": lr_init\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in param_optimizer if \"heads\" in n],\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"lr\": lr_init * 10  # Higher learning rate for classification head\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    return optimizer_parameters\n",
    "\n",
    "# Initialize model and training parameters\n",
    "print(\"Initializing ViT model...\")\n",
    "vit_model = create_vit_model(num_classes)\n",
    "# Use class_weights if computed; fall back to unweighted if not\n",
    "try:\n",
    "    class_weights_tensor = class_weights.to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "    print('Using class-weighted CrossEntropyLoss for ViT')\n",
    "except NameError:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    print('Class weights not found; using standard CrossEntropyLoss')\n",
    "\n",
    "# Set up optimizer with layer-wise learning rate decay\n",
    "optimizer_params = get_optimizer_params(\n",
    "    vit_model,\n",
    "    weight_decay=0.01,\n",
    "    lr_init=1e-4\n",
    ")\n",
    "optimizer = optim.AdamW(optimizer_params)\n",
    "\n",
    "# Create scheduler with warm-up and cosine decay\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=len(train_loader) * 2,  # 2 epochs of warmup\n",
    "    num_training_steps=len(train_loader) * num_epochs\n",
    ")\n",
    "\n",
    "# Training configuration\n",
    "print(\"\\nTraining Configuration:\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Number of epochs: {num_epochs}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Training loop\n",
    "print(\"\\nStarting ViT training...\")\n",
    "best_val_f1 = 0\n",
    "train_losses, val_losses = [], []\n",
    "train_f1s, val_f1s = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    # Training phase\n",
    "    vit_model.train()\n",
    "    train_loss = 0\n",
    "    train_preds, train_labels = [], []\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = vit_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(vit_model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_preds.extend(predicted.cpu().numpy())\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "    # Calculate training metrics\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    train_f1 = f1_score(train_labels, train_preds, average='macro')\n",
    "    \n",
    "    # Validation phase\n",
    "    vit_model.eval()\n",
    "    val_loss = 0\n",
    "    val_preds, val_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = vit_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_preds.extend(predicted.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate validation metrics\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
    "    \n",
    "    # Store metrics\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_f1s.append(train_f1)\n",
    "    val_f1s.append(val_f1)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        print(\"Saving best model...\")\n",
    "        torch.save(vit_model.state_dict(), '/kaggle/working/models/vit_best.pth')\n",
    "    \n",
    "    # Print epoch metrics\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Macro-F1: {train_f1:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Macro-F1: {val_f1:.4f}\")\n",
    "    \n",
    "    # Print learning rates\n",
    "    print(\"Learning rates:\")\n",
    "    for i, param_group in enumerate(optimizer.param_groups):\n",
    "        print(f\"Group {i}: {param_group['lr']:.6f}\")\n",
    "\n",
    "# Plot training progress\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.title('ViT Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# F1 score plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_f1s, label='Train Macro-F1')\n",
    "plt.plot(val_f1s, label='Val Macro-F1')\n",
    "plt.title('ViT Training and Validation Macro-F1')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Macro-F1 Score')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVision Transformer training completed!\")\n",
    "print(f\"Best validation Macro-F1: {best_val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f56c8d1",
   "metadata": {},
   "source": [
    "## 4. Results and Analysis\n",
    "\n",
    "### 4.1 Experimental Setup\n",
    "- Hardware: Kaggle GPU runtime\n",
    "- Software: PyTorch, torchvision, Ultralytics\n",
    "- Metrics: Accuracy, Macro-F1, Inference Time\n",
    "- Cross-validation: Stratified validation split\n",
    "\n",
    "### 4.2 Quantitative Analysis\n",
    "We evaluate models on multiple metrics:\n",
    "1. Macro-F1 scores for balanced performance\n",
    "2. Per-class precision and recall\n",
    "3. Confusion matrices\n",
    "4. Inference speed benchmarks\n",
    "\n",
    "### 4.3 Comparative Analysis\n",
    "Detailed comparison of models across:\n",
    "- Overall performance metrics\n",
    "- Disease-specific accuracy\n",
    "- Computational efficiency\n",
    "- Model robustness\n",
    "\n",
    "The following section presents our comprehensive evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb0856d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation utilities\n",
    "def evaluate_model(model, loader, name=\"\"):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    inference_times = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(loader, desc=f\"Evaluating {name}\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Measure inference time\n",
    "            start_time = time.time()\n",
    "            outputs = model(inputs)\n",
    "            inference_times.append(time.time() - start_time)\n",
    "            \n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            _, preds = outputs.max(1)\n",
    "            \n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    report = classification_report(all_labels, all_preds, \n",
    "                                target_names=list(label_names.values()), \n",
    "                                output_dict=True)\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    avg_inference_time = np.mean(inference_times) * 1000  # Convert to ms\n",
    "    \n",
    "    return report, cm, avg_inference_time, np.array(all_probs), np.array(all_labels)\n",
    "\n",
    "# Evaluate CNN models\n",
    "models = {\n",
    "    'ResNet50': resnet_model,\n",
    "    'EfficientNet': efficient_model\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    report, cm, inf_time, probs, labels = evaluate_model(model, val_loader, name)\n",
    "    results[name] = {\n",
    "        'report': report,\n",
    "        'confusion_matrix': cm,\n",
    "        'inference_time': inf_time,\n",
    "        'probabilities': probs,\n",
    "        'true_labels': labels\n",
    "    }\n",
    "\n",
    "# Add YOLO results\n",
    "yolo_metrics = yolo_model.val()\n",
    "results['YOLOv8'] = {\n",
    "    'report': yolo_metrics.results_dict,\n",
    "    'confusion_matrix': yolo_metrics.confusion_matrix.matrix,\n",
    "    'inference_time': yolo_metrics.speed['inference']\n",
    "}\n",
    "\n",
    "# Create comparison visualizations\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# 1. Macro F1-Score Comparison\n",
    "plt.subplot(131)\n",
    "macro_f1_scores = []\n",
    "for model in models:\n",
    "    macro_f1_scores.append(results[model]['report']['macro avg']['f1-score'] * 100)\n",
    "macro_f1_scores.append(results['YOLOv8']['report']['metrics/f1'] * 100)  # YOLO's F1\n",
    "\n",
    "plt.bar(['ResNet50', 'EfficientNet', 'YOLOv8'], macro_f1_scores)\n",
    "plt.title('Macro F1-Score Comparison')\n",
    "plt.ylabel('Macro F1-Score (%)')\n",
    "plt.ylim(0, 100)\n",
    "\n",
    "# 2. Accuracy Comparison\n",
    "plt.subplot(132)\n",
    "accuracies = [results[model]['report']['accuracy'] * 100 for model in models]\n",
    "accuracies.append(results['YOLOv8']['report']['metrics/accuracy'] * 100)\n",
    "plt.bar(['ResNet50', 'EfficientNet', 'YOLOv8'], accuracies)\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.ylim(0, 100)\n",
    "\n",
    "# 3. Inference Time Comparison\n",
    "plt.subplot(133)\n",
    "inf_times = [results[model]['inference_time'] for model in list(models.keys()) + ['YOLOv8']]\n",
    "plt.bar(['ResNet50', 'EfficientNet', 'YOLOv8'], inf_times)\n",
    "plt.title('Inference Time Comparison')\n",
    "plt.ylabel('Time per batch (ms)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot per-class F1 scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "disease_names = [name[:15] for name in label_names.values()]\n",
    "x = np.arange(len(disease_names))\n",
    "width = 0.25\n",
    "\n",
    "for i, (name, model_results) in enumerate(results.items()):\n",
    "    if name != 'YOLOv8':\n",
    "        f1_scores = [model_results['report'][cls]['f1-score'] * 100 for cls in label_names.values()]\n",
    "    else:\n",
    "        # Assuming YOLO provides per-class metrics\n",
    "        f1_scores = [model_results['report'].get(f'metrics/f1_class{i}', 0) * 100 for i in range(len(disease_names))]\n",
    "    \n",
    "    plt.bar(x + i*width, f1_scores, width, label=name)\n",
    "\n",
    "plt.xlabel('Disease Categories')\n",
    "plt.ylabel('F1-Score (%)')\n",
    "plt.title('Per-Class F1 Scores')\n",
    "plt.xticks(x + width, disease_names, rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot confusion matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "for idx, (name, model_results) in enumerate(results.items()):\n",
    "    sns.heatmap(model_results['confusion_matrix'], \n",
    "                annot=True, \n",
    "                fmt='d',\n",
    "                ax=axes[idx],\n",
    "                cmap='Blues',\n",
    "                xticklabels=[name[:3] for name in label_names.values()],\n",
    "                yticklabels=[name[:3] for name in label_names.values()])\n",
    "    axes[idx].set_title(f'{name} Confusion Matrix')\n",
    "    axes[idx].set_xlabel('Predicted')\n",
    "    axes[idx].set_ylabel('True')\n",
    "    plt.setp(axes[idx].get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed results with emphasis on macro-F1\n",
    "print(\"\\nDetailed Model Comparison:\")\n",
    "print(\"-\" * 80)\n",
    "comparison_data = []\n",
    "for name, model_results in results.items():\n",
    "    if name != 'YOLOv8':\n",
    "        macro_f1 = model_results['report']['macro avg']['f1-score'] * 100\n",
    "        accuracy = model_results['report']['accuracy'] * 100\n",
    "        weighted_f1 = model_results['report']['weighted avg']['f1-score'] * 100\n",
    "    else:\n",
    "        macro_f1 = results['YOLOv8']['report']['metrics/f1'] * 100\n",
    "        accuracy = results['YOLOv8']['report']['metrics/accuracy'] * 100\n",
    "        weighted_f1 = results['YOLOv8']['report'].get('metrics/f1_weighted', macro_f1)\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Model': name,\n",
    "        'Macro F1-Score (%)': f\"{macro_f1:.2f}\",\n",
    "        'Accuracy (%)': f\"{accuracy:.2f}\",\n",
    "        'Weighted F1-Score (%)': f\"{weighted_f1:.2f}\",\n",
    "        'Inference Time (ms)': f\"{model_results['inference_time']:.2f}\"\n",
    "    })\n",
    "\n",
    "# Display comparison table\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Save detailed results\n",
    "comparison_df.to_csv('/kaggle/working/model_comparison_results.csv', index=False)\n",
    "print(\"\\nDetailed results saved to 'model_comparison_results.csv'\")\n",
    "\n",
    "# Print per-class performance\n",
    "print(\"\\nPer-Class Performance:\")\n",
    "print(\"-\" * 80)\n",
    "for name, model_results in results.items():\n",
    "    if name != 'YOLOv8':\n",
    "        print(f\"\\n{name} Per-Class Metrics:\")\n",
    "        class_metrics = pd.DataFrame({\n",
    "            'F1-Score (%)': {k: v['f1-score']*100 for k, v in model_results['report'].items() \n",
    "                            if k in label_names.values()},\n",
    "            'Precision (%)': {k: v['precision']*100 for k, v in model_results['report'].items() \n",
    "                             if k in label_names.values()},\n",
    "            'Recall (%)': {k: v['recall']*100 for k, v in model_results['report'].items() \n",
    "                          if k in label_names.values()}\n",
    "        })\n",
    "        print(class_metrics.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440a5228",
   "metadata": {},
   "source": [
    "## 5. Discussion and Future Work\n",
    "\n",
    "### 5.1 Key Findings\n",
    "Our experimental results reveal several important insights:\n",
    "1. Model Performance Trade-offs\n",
    "   - Accuracy vs. computational efficiency\n",
    "   - Architecture-specific strengths\n",
    "   - Disease-specific detection capabilities\n",
    "\n",
    "2. Technical Contributions\n",
    "   - Effective training strategies\n",
    "   - Optimal hyperparameter configurations\n",
    "   - Performance optimization techniques\n",
    "\n",
    "### 5.2 Limitations\n",
    "Current limitations include:\n",
    "- Dataset constraints\n",
    "- Computational resource requirements\n",
    "- Real-world deployment considerations\n",
    "\n",
    "### 5.3 Future Directions\n",
    "Potential areas for future research:\n",
    "1. Model Improvements\n",
    "   - Ensemble methods\n",
    "   - Architecture modifications\n",
    "   - Optimization techniques\n",
    "\n",
    "2. Practical Applications\n",
    "   - Mobile deployment\n",
    "   - Real-time processing\n",
    "   - Edge device implementation\n",
    "\n",
    "### 5.4 Conclusion\n",
    "This study demonstrates the effectiveness of deep learning approaches in cassava disease classification, with each architecture offering unique advantages. The results provide valuable insights for implementing similar systems in agricultural applications.\n",
    "\n",
    "### References\n",
    "1. He, K., et al. (2016). Deep Residual Learning for Image Recognition\n",
    "2. Tan, M., & Le, Q. (2019). EfficientNet: Rethinking Model Scaling for CNNs\n",
    "3. Jocher, G., et al. (2023). Ultralytics YOLOv8"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
